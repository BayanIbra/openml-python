<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Basic Usage &#8212; OpenML 0.6.0dev documentation</title>
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.6.0dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body>
  
  <a href="https://github.com/openml/openml-python"
     class="visible-desktop hidden-xs"><img
    id="gh-banner"
    style="position: absolute; top: 50px; right: 0; border: 0;"
    src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png"
    alt="Fork me on GitHub"></a>
  <script>
    // Adjust banner height.
    $(function () {
      var navHeight = $(".navbar .container").css("height");
      $("#gh-banner").css("top", navHeight);
    });
  </script>


  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          OpenML</a>
        <span class="navbar-text navbar-version pull-left"><b>0.6.0dev</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="index.html">Start</a></li>
                <li><a href="api.html">API</a></li>
                <li><a href="#">User Guide</a></li>
                <li><a href="progress.html">Progress</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary"><ul>
<li><a class="reference internal" href="#">Basic Usage</a><ul>
<li><a class="reference internal" href="#connecting-to-the-openml-server">Connecting to the OpenML server</a></li>
<li><a class="reference internal" href="#working-with-datasets">Working with datasets</a><ul>
<li><a class="reference internal" href="#listing-datasets">Listing datasets</a></li>
<li><a class="reference internal" href="#downloading-datasets">Downloading datasets</a></li>
</ul>
</li>
<li><a class="reference internal" href="#working-with-tasks">Working with tasks</a><ul>
<li><a class="reference internal" href="#listing-tasks">Listing tasks</a></li>
<li><a class="reference internal" href="#downloading-tasks">Downloading tasks</a></li>
</ul>
</li>
<li><a class="reference internal" href="#finding-out-tasks-types">Finding out tasks types</a></li>
<li><a class="reference internal" href="#finding-out-evaluation-strategies-and-target-metrics">Finding out evaluation strategies and target metrics</a></li>
<li><a class="reference internal" href="#using-the-cache">Using the cache</a><ul>
<li><a class="reference internal" href="#configuring-the-cache">Configuring the cache</a></li>
<li><a class="reference internal" href="#clearing-the-cache">Clearing the cache</a></li>
</ul>
</li>
<li><a class="reference internal" href="#working-with-flows-and-runs">Working with Flows and Runs</a><ul>
<li><a class="reference internal" href="#running-a-model">Running a model</a></li>
<li><a class="reference internal" href="#retrieving-results-from-openml">Retrieving results from OpenML</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    <div class="col-md-9 content">
      
  <div class="section" id="basic-usage">
<span id="usage"></span><h1>Basic Usage<a class="headerlink" href="#basic-usage" title="Permalink to this headline">¶</a></h1>
<p>This document will guide you through the most important functions and classes
in the OpenML Python API. Throughout this document, we will use
<a class="reference external" href="http://pandas.pydata.org/">pandas</a> to format and filter tables.</p>
<div class="section" id="connecting-to-the-openml-server">
<h2>Connecting to the OpenML server<a class="headerlink" href="#connecting-to-the-openml-server" title="Permalink to this headline">¶</a></h2>
<p>The OpenML server can only be accessed by users who have signed up on the OpenML
platform. If you don’t have an account yet,
<a class="reference external" href="http://openml.org/register">sign up now</a>. You will receive an API key, which
will authenticate you to the server and allow you to download and upload
datasets, tasks, runs and flows. There are two ways of providing the API key
to the OpenML API package. The first option is to specify the API key
programmatically after loading the package:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">openml</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">apikey</span> <span class="o">=</span> <span class="s1">&#39;Your API key&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">openml</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">apikey</span> <span class="o">=</span> <span class="n">apikey</span>
</pre></div>
</div>
<p>The second option is to create a config file:</p>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="n">apikey</span> <span class="o">=</span> <span class="n">qxlfpbeaudtprb23985hcqlfoebairtd</span>
</pre></div>
</div>
<p>The config file must be in the directory <code class="code bash docutils literal"><span class="pre">~/.openml/config</span></code> and
exist prior to importing the openml module.</p>
<p>When downloading datasets, tasks, runs and flows, they will be cached to
retrieve them without calling the server later. As with the API key, the cache
directory can be either specified through the API or through the config file:</p>
<p>API:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">os</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">openml</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">set_cache_directory</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/.openml/cache&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>Config file:</p>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="n">cachedir</span> <span class="o">=</span> <span class="s1">&#39;~/.openml/cache&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="working-with-datasets">
<h2>Working with datasets<a class="headerlink" href="#working-with-datasets" title="Permalink to this headline">¶</a></h2>
<p># TODO mention third, searching for tags</p>
<p>Datasets are a key concept in OpenML (see <a class="reference external" href="openml.org/guide">OpenML documentation</a>).
Datasets are identified by IDs and can be accessed in two different ways:</p>
<ol class="arabic simple">
<li>In a list providing basic information on all datasets available on OpenML.
This function will not download the actual dataset, but will instead download
meta data which can be used to filter the datasets and retrieve a set of IDs.</li>
<li>A single dataset by its ID. A single dataset contains all meta information and the actual
data in form of an .arff file. The .arff file will be converted into a numpy
array by the OpenML Python API.</li>
</ol>
<div class="section" id="listing-datasets">
<h3>Listing datasets<a class="headerlink" href="#listing-datasets" title="Permalink to this headline">¶</a></h3>
<p>A common task when using OpenML is to find a set of datasets which fulfill
several criteria. They should for example have between 1,000 and 10,000
data points and at least five features.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">datasets</span> <span class="o">=</span> <span class="n">openml</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">list_datasets</span><span class="p">()</span>
</pre></div>
</div>
<p><a class="reference internal" href="generated/openml.datasets.list_datasets.html#openml.datasets.list_datasets" title="openml.datasets.list_datasets"><code class="xref py py-meth docutils literal"><span class="pre">openml.datasets.list_datasets()</span></code></a> returns a dictionary of dictionaries, we
will convert it into a
<a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html">pandas dataframe</a>
to have better visualization and easier access:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">datasets</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">datasets</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s1">&#39;index&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>We have access to the following properties of the datasets:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">datasets</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="go">Index([&#39;did&#39;, &#39;name&#39;, &#39;format&#39;, &#39;status&#39;, &#39;MajorityClassSize&#39;,</span>
<span class="go">       &#39;MaxNominalAttDistinctValues&#39;, &#39;MinorityClassSize&#39;, &#39;NumberOfClasses&#39;,</span>
<span class="go">       &#39;NumberOfFeatures&#39;, &#39;NumberOfInstances&#39;,</span>
<span class="go">       &#39;NumberOfInstancesWithMissingValues&#39;, &#39;NumberOfMissingValues&#39;,</span>
<span class="go">       &#39;NumberOfNumericFeatures&#39;, &#39;NumberOfSymbolicFeatures&#39;],</span>
<span class="go">      dtype=&#39;object&#39;)</span>
</pre></div>
</div>
<p>and can see the first data point:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">datasets</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="go">did                                        2</span>
<span class="go">name                                  anneal</span>
<span class="go">format                                  ARFF</span>
<span class="go">status                                active</span>
<span class="go">MajorityClassSize                        684</span>
<span class="go">MaxNominalAttDistinctValues                7</span>
<span class="go">MinorityClassSize                          8</span>
<span class="go">NumberOfClasses                            5</span>
<span class="go">NumberOfFeatures                          39</span>
<span class="go">NumberOfInstances                        898</span>
<span class="go">NumberOfInstancesWithMissingValues       898</span>
<span class="go">NumberOfMissingValues                  22175</span>
<span class="go">NumberOfNumericFeatures                    6</span>
<span class="go">NumberOfSymbolicFeatures                  33</span>
<span class="go">Name: 2, dtype: object</span>
</pre></div>
</div>
<p>We can now filter the data:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">filter</span> <span class="o">=</span> <span class="p">(</span><span class="n">datasets</span><span class="o">.</span><span class="n">NumberOfInstances</span> <span class="o">&gt;</span> <span class="mi">1000</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">datasets</span><span class="o">.</span><span class="n">NumberOfFeatures</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filtered_datasets</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="nb">filter</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">filtered_datasets</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">dataset_indices</span><span class="p">)</span>                                                  
<span class="go">[3, 6, 12, 14, 16, 18, 20, 21, 22, 23, 24, 26, 28, 30, 32, 36, 38, 44,</span>
<span class="gp">... </span><span class="mi">5291</span><span class="p">,</span> <span class="mi">5293</span><span class="p">,</span> <span class="mi">5295</span><span class="p">,</span> <span class="mi">5296</span><span class="p">,</span> <span class="mi">5297</span><span class="p">,</span> <span class="mi">5301</span><span class="p">,</span> <span class="mi">5587</span><span class="p">,</span> <span class="mi">5648</span><span class="p">,</span> <span class="mi">5889</span><span class="p">]</span>
</pre></div>
</div>
<p>and get a list of dataset indices which can be used in a next step.</p>
</div>
<div class="section" id="downloading-datasets">
<h3>Downloading datasets<a class="headerlink" href="#downloading-datasets" title="Permalink to this headline">¶</a></h3>
<p>We can now use the dataset IDs to download all datasets by their IDs. Let’s
first look at how to download a single dataset and what can be done with the
dataset object:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dataset_id</span> <span class="o">=</span> <span class="mi">23</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">openml</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">get_dataset</span><span class="p">(</span><span class="n">dataset_id</span><span class="p">)</span>
</pre></div>
</div>
<p>Properties of the dataset are stored as member variables:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>                                               
<span class="go">{&#39;upload_date&#39;: u&#39;2014-04-06 23:21:03&#39;, &#39;md5_cheksum&#39;: u&#39;3149646ecff276abac3e892d1556655f&#39;, &#39;creator&#39;: None, &#39;citation&#39;: None, &#39;tag&#39;: [u&#39;study_1&#39;, u&#39;study_7&#39;, u&#39;uci&#39;], &#39;version_label&#39;: u&#39;1&#39;, &#39;contributor&#39;: None, &#39;paper_url&#39;: None, &#39;original_data_url&#39;: None, &#39;id&#39;: 23, &#39;collection_date&#39;: None, &#39;row_id_attribute&#39;: None, &#39;version&#39;: 1, &#39;data_pickle_file&#39;: &#39;/home/matthias/.openml/cache/datasets/23/dataset.pkl&#39;, &#39;default_target_attribute&#39;: u&#39;Contraceptive_method_used&#39;, &#39;description&#39;: u&quot;**Author**:   \n**Source**: Unknown -   \n**Please cite**:   \n\n1. Title: Contraceptive Method Choice\n \n 2. Sources:\n    (a) Origin:  This dataset is a subset of the 1987 National Indonesia\n                 Contraceptive Prevalence Survey\n    (b) Creator: Tjen-Sien Lim (limt@stat.wisc.edu)\n    (c) Donor:   Tjen-Sien Lim (limt@stat.wisc.edu)\n    (c) Date:    June 7, 1997\n \n 3. Past Usage:\n    Lim, T.-S., Loh, W.-Y. &amp; Shih, Y.-S. (1999). A Comparison of\n    Prediction Accuracy, Complexity, and Training Time of Thirty-three\n    Old and New Classification Algorithms. Machine Learning. Forthcoming.\n    (ftp://ftp.stat.wisc.edu/pub/loh/treeprogs/quest1.7/mach1317.pdf or\n    (http://www.stat.wisc.edu/~limt/mach1317.pdf)\n \n 4. Relevant Information:\n    This dataset is a subset of the 1987 National Indonesia Contraceptive\n    Prevalence Survey. The samples are married women who were either not \n    pregnant or do not know if they were at the time of interview. The \n    problem is to predict the current contraceptive method choice \n    (no use, long-term methods, or short-term methods) of a woman based \n    on her demographic and socio-economic characteristics.\n \n 5. Number of Instances: 1473\n \n 6. Number of Attributes: 10 (including the class attribute)\n \n 7. Attribute Information:\n \n    1. Wife&#39;s age                     (numerical)\n    2. Wife&#39;s education               (categorical)      1=low, 2, 3, 4=high\n    3. Husband&#39;s education            (categorical)      1=low, 2, 3, 4=high\n    4. Number of children ever born   (numerical)\n    5. Wife&#39;s religion                (binary)           0=Non-Islam, 1=Islam\n    6. Wife&#39;s now working?            (binary)           0=Yes, 1=No\n    7. Husband&#39;s occupation           (categorical)      1, 2, 3, 4\n    8. Standard-of-living index       (categorical)      1=low, 2, 3, 4=high\n    9. Media exposure                 (binary)           0=Good, 1=Not good\n    10. Contraceptive method used     (class attribute)  1=No-use \n                                                         2=Long-term\n                                                         3=Short-term\n \n 8. Missing Attribute Values: None\n\n Information about the dataset\n CLASSTYPE: nominal\n CLASSINDEX: last&quot;, &#39;format&#39;: u&#39;ARFF&#39;, &#39;visibility&#39;: u&#39;public&#39;, &#39;update_comment&#39;: None, &#39;licence&#39;: u&#39;Public&#39;, &#39;name&#39;: u&#39;cmc&#39;, &#39;language&#39;: None, &#39;url&#39;: u&#39;http://www.openml.org/data/download/23/dataset_23_cmc.arff&#39;, &#39;data_file&#39;: &#39;~/.openml/cache/datasets/23/dataset.arff&#39;, &#39;ignore_attributes&#39;: None}</span>
</pre></div>
</div>
<p>Next, to obtain the data matrix:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_data</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="go">(1473, 10) float32</span>
</pre></div>
</div>
<p>which returns the dataset as a np.ndarray with dtype <code class="code python docutils literal"><span class="name"><span class="pre">np</span></span><span class="operator"><span class="pre">.</span></span><span class="name"><span class="pre">float32</span></span></code>.
In case the data is sparse, a scipy.sparse.csr matrix is returned. All nominal
variables are encoded as integers, the inverse encoding can be retrieved via:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">names</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_data</span><span class="p">(</span><span class="n">return_attribute_names</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
<span class="go">[&#39;Wifes_age&#39;, &#39;Wifes_education&#39;, &#39;Husbands_education&#39;, &#39;Number_of_children_ever_born&#39;, &#39;Wifes_religion&#39;, &#39;Wifes_now_working%3F&#39;, &#39;Husbands_occupation&#39;, &#39;Standard-of-living_index&#39;, &#39;Media_exposure&#39;, &#39;Contraceptive_method_used&#39;]</span>
</pre></div>
</div>
<p>Most times, having a single data matrix <code class="code python docutils literal"><span class="name"><span class="pre">X</span></span></code> is not enough. Two
useful arguments are <code class="code python docutils literal"><span class="name"><span class="pre">target</span></span></code> and
<code class="code python docutils literal"><span class="name"><span class="pre">return_categorical_indicator</span></span></code>. <code class="code python docutils literal"><span class="name"><span class="pre">target</span></span></code> makes
<code class="xref py py-meth docutils literal"><span class="pre">get_data()</span></code> return <code class="code python docutils literal"><span class="name"><span class="pre">X</span></span></code> and <code class="code python docutils literal"><span class="name"><span class="pre">y</span></span></code>
seperate; <code class="code python docutils literal"><span class="name"><span class="pre">return_categorical_indicator</span></span></code> makes
<code class="xref py py-meth docutils literal"><span class="pre">get_data()</span></code> return a boolean array which indicate
which attributes are categorical (and should be one hot encoded if necessary.)</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">categorical</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_data</span><span class="p">(</span>
<span class="gp">... </span><span class="n">target</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">default_target_attribute</span><span class="p">,</span>
<span class="gp">... </span><span class="n">return_categorical_indicator</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(1473, 9) (1473,)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">categorical</span><span class="p">)</span>
<span class="go">[False, True, True, False, True, True, True, True, True]</span>
</pre></div>
</div>
<p>In case you are working with <a class="reference external" href="http://scikit-learn">scikit-learn</a>, you can use this data right away:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="n">ensemble</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">categorical_features</span><span class="o">=</span><span class="n">categorical</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span>
<span class="go">OneHotEncoder(categorical_features=[False, True, True, False, True, True, True, True, True],</span>
<span class="go">       dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;error&#39;,</span>
<span class="go">       n_values=&#39;auto&#39;, sparse=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,</span>
<span class="go">            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,</span>
<span class="go">            min_impurity_split=1e-07, min_samples_leaf=1,</span>
<span class="go">            min_samples_split=2, min_weight_fraction_leaf=0.0,</span>
<span class="go">            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,</span>
<span class="go">            verbose=0, warm_start=False)</span>
</pre></div>
</div>
<p>When you have to retrieve several datasets, you can use the convenience function
<a class="reference internal" href="generated/openml.datasets.get_datasets.html#openml.datasets.get_datasets" title="openml.datasets.get_datasets"><code class="xref py py-meth docutils literal"><span class="pre">openml.datasets.get_datasets()</span></code></a>, which downloads all datasets given by
a list of IDs:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">22</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">datasets</span> <span class="o">=</span> <span class="n">openml</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">get_datasets</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">datasets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go">mfeat-factors</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="working-with-tasks">
<h2>Working with tasks<a class="headerlink" href="#working-with-tasks" title="Permalink to this headline">¶</a></h2>
<p>#TODO put a link to the OpenML documentation here! Link the Task functions and
the task class</p>
<p>While datasets provide the most basic information for a machine learning task,
they do not provide enough information for a reproducible machine learning
experiment. A task defines how to split the dataset into a train and test set,
whether to use several disjoint train and test splits (cross-validation) and
whether this should be repeated several times. Also, the task defines a target
metric for which a flow should be optimized.</p>
<p>Just like datasets, tasks are identified by IDs and can be accessed in three
different ways:</p>
<ol class="arabic simple">
<li>In a list providing basic information on all tasks available on OpenML.
This function will not download the actual tasks, but will instead download
meta data that can be used to filter the tasks and retrieve a set of IDs.</li>
<li>By functions only list a subset of all available tasks, restricted either by
their <a href="#id1"><span class="problematic" id="id2">:TODO:`task_type`</span></a>, <a href="#id3"><span class="problematic" id="id4">:TODO:`tag`</span></a> or <a href="#id5"><span class="problematic" id="id6">:TODO:`check_for_more`</span></a>.</li>
<li>A single task by its ID. It contains all meta information, the target metric,
the splits and an iterator which can be used to access the splits in a
useful manner.</li>
</ol>
<p>You can also read more about tasks in the <a class="reference external" href="http://www.openml.org/guide">OpenML guide</a>.</p>
<div class="section" id="listing-tasks">
<h3>Listing tasks<a class="headerlink" href="#listing-tasks" title="Permalink to this headline">¶</a></h3>
<p>Once we decide on the datasets we want to work on, we have to download the
corresponding tasks. Tasks can be pre-filtered by either by a task type or
a tag.</p>
<p>So far, this package only supports supervised classification tasks (task
type <code class="code python docutils literal"><span class="literal number integer"><span class="pre">1</span></span></code>) and supervised regression tasks (task type <code class="code python docutils literal"><span class="literal number integer"><span class="pre">2</span></span></code>) #TODO check this
We desribe how to find other task types in the subsection <a href="#id9"><span class="problematic" id="id10">`Finding out task types`_</span></a>
and are happy to receive contributions that help us to support all other task
types.</p>
<p>The most natural way to retrieve tasks is by their task type. In this example
we will use the most commonly studied machine learning supervised
classification task (task type <code class="code python docutils literal"><span class="literal number integer"><span class="pre">1</span></span></code>):</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tasks</span> <span class="o">=</span> <span class="n">openml</span><span class="o">.</span><span class="n">tasks</span><span class="o">.</span><span class="n">list_tasks</span><span class="p">(</span><span class="n">task_type_id</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s find out more about the datasets:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tasks</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">tasks</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s1">&#39;index&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tasks</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="go">Index([&#39;tid&#39;, &#39;ttid&#39;, &#39;did&#39;, &#39;name&#39;, &#39;task_type&#39;, &#39;status&#39;,</span>
<span class="go">       &#39;estimation_procedure&#39;, &#39;evaluation_measures&#39;, &#39;source_data&#39;,</span>
<span class="go">       &#39;target_feature&#39;, &#39;MajorityClassSize&#39;, &#39;MaxNominalAttDistinctValues&#39;,</span>
<span class="go">       &#39;MinorityClassSize&#39;, &#39;NumberOfClasses&#39;, &#39;NumberOfFeatures&#39;,</span>
<span class="go">       &#39;NumberOfInstances&#39;, &#39;NumberOfInstancesWithMissingValues&#39;,</span>
<span class="go">       &#39;NumberOfMissingValues&#39;, &#39;NumberOfNumericFeatures&#39;,</span>
<span class="go">       &#39;NumberOfSymbolicFeatures&#39;, &#39;cost_matrix&#39;],</span>
<span class="go">      dtype=&#39;object&#39;)</span>
</pre></div>
</div>
<p>Now we can restrict the tasks to all tasks with the desired resampling strategy:</p>
<p># TODO add something about the different resampling strategies implemented!</p>
<p>Resampling strategies can be found on the <a class="reference external" href="http://www.openml.org/search?type=measure&amp;q=estimation%20procedure">OpenML Website</a>
or programatically as described in <a class="reference internal" href="#finding-out-evaluation-strategies-and-target-metrics">Finding out evaluation strategies and target metrics</a>.</p>
<p>Finally, we can check whether there is a task for each dataset that we want to
use in our study. If this is not the case, tasks can be created on the
<a class="reference external" href="openml.org/tasks/create">OpenML website</a>.</p>
<p>The rest of this subsection deals with accessing a list of tasks by tags and
without any restriction.</p>
<p>A list of tasks, filtered tags, can be retrieved via:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tasks</span> <span class="o">=</span> <span class="n">openml</span><span class="o">.</span><span class="n">tasks</span><span class="o">.</span><span class="n">list_tasks</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;study_1&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="generated/openml.tasks.list_tasks.html#openml.tasks.list_tasks" title="openml.tasks.list_tasks"><code class="xref py py-meth docutils literal"><span class="pre">openml.tasks.list_tasks()</span></code></a> returns a dict of dictionaries, we will
convert it into a <a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html">pandas dataframe</a>
to have better visualization:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tasks</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">tasks</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s1">&#39;index&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>As before, we have to check whether there is a task for each dataset that we
want to work with. In addition, we have to make sure to use only tasks with the
desired task type:</p>
<p>#TODO this doesn’t look nice, we should have a constant for each known task,
dynamically created by the task type available (but when do we know that we
can savely use the api connector? what to do if we do not have an internet
connection? Maybe have this statically in the program and check from time to
time if there is something new (via a unit test?)?, the same holds true for
the resampling strategies available!)</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">filter</span> <span class="o">=</span> <span class="n">tasks</span><span class="o">.</span><span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;Supervised Classification&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filtered_tasks</span> <span class="o">=</span> <span class="n">tasks</span><span class="p">[</span><span class="nb">filter</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">filtered_tasks</span><span class="p">))</span>                                  
<span class="go">2599</span>
</pre></div>
</div>
<p>Finally, it is also possible to list all tasks on OpenML with:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tasks</span> <span class="o">=</span> <span class="n">openml</span><span class="o">.</span><span class="n">tasks</span><span class="o">.</span><span class="n">list_tasks</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tasks</span><span class="p">))</span>                       
<span class="go">29757</span>
</pre></div>
</div>
</div>
<div class="section" id="downloading-tasks">
<h3>Downloading tasks<a class="headerlink" href="#downloading-tasks" title="Permalink to this headline">¶</a></h3>
<p>Downloading tasks works similar to downloading datasets. We provide two
functions for this, one which downloads only a single task by its ID,
and one which takes a list of IDs and downloads all of these tasks:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">task_id</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task</span> <span class="o">=</span> <span class="n">openml</span><span class="o">.</span><span class="n">tasks</span><span class="o">.</span><span class="n">get_task</span><span class="p">(</span><span class="n">task_id</span><span class="p">)</span>
</pre></div>
</div>
<p>Properties of the task are stored as member variables:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pprint</span> <span class="k">import</span> <span class="n">pprint</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pprint</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="n">task</span><span class="p">))</span>
<span class="go">{&#39;class_labels&#39;: [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;U&#39;],</span>
<span class="go"> &#39;cost_matrix&#39;: None,</span>
<span class="go"> &#39;dataset_id&#39;: 2,</span>
<span class="go"> &#39;estimation_parameters&#39;: {&#39;number_folds&#39;: &#39;10&#39;,</span>
<span class="go">                           &#39;number_repeats&#39;: &#39;1&#39;,</span>
<span class="go">                           &#39;percentage&#39;: &#39;&#39;,</span>
<span class="go">                           &#39;stratified_sampling&#39;: &#39;true&#39;},</span>
<span class="go"> &#39;estimation_procedure&#39;: {&#39;data_splits_url&#39;: &#39;https://www.openml.org/api_splits/get/2/Task_2_splits.arff&#39;,</span>
<span class="go">                          &#39;parameters&#39;: {&#39;number_folds&#39;: &#39;10&#39;,</span>
<span class="go">                                         &#39;number_repeats&#39;: &#39;1&#39;,</span>
<span class="go">                                         &#39;percentage&#39;: &#39;&#39;,</span>
<span class="go">                                         &#39;stratified_sampling&#39;: &#39;true&#39;},</span>
<span class="go">                          &#39;type&#39;: &#39;crossvalidation&#39;},</span>
<span class="go"> &#39;evaluation_measure&#39;: &#39;predictive_accuracy&#39;,</span>
<span class="go"> &#39;split&#39;: None,</span>
<span class="go"> &#39;target_name&#39;: &#39;class&#39;,</span>
<span class="go"> &#39;task_id&#39;: 2,</span>
<span class="go"> &#39;task_type&#39;: &#39;Supervised Classification&#39;,</span>
<span class="go"> &#39;task_type_id&#39;: 1}</span>
</pre></div>
</div>
<p>And with a list of task IDs:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">22</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tasks</span> <span class="o">=</span> <span class="n">openml</span><span class="o">.</span><span class="n">tasks</span><span class="o">.</span><span class="n">get_tasks</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pprint</span><span class="p">(</span><span class="n">tasks</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>                           
</pre></div>
</div>
</div>
</div>
<div class="section" id="finding-out-tasks-types">
<h2>Finding out tasks types<a class="headerlink" href="#finding-out-tasks-types" title="Permalink to this headline">¶</a></h2>
<p>Not yet supported by the API. Please use the OpenML website.</p>
</div>
<div class="section" id="finding-out-evaluation-strategies-and-target-metrics">
<h2>Finding out evaluation strategies and target metrics<a class="headerlink" href="#finding-out-evaluation-strategies-and-target-metrics" title="Permalink to this headline">¶</a></h2>
<p>Not yet supported by the API. Please use the OpenML website.</p>
</div>
<div class="section" id="using-the-cache">
<h2>Using the cache<a class="headerlink" href="#using-the-cache" title="Permalink to this headline">¶</a></h2>
<p>Downloading all datasets, tasks and split every time a get function is called
would prohibit a user to interact with the API in an exploratory manner.
OpenML is designed in a way that certain entities are immutable once created.
This allows the python package to cache datasets, tasks, splits and runs locally
for fast retrieval. Another benefit is that the API can be used normally on a
compute cluster without internet access (<span class="xref std std-ref">see below</span>).</p>
<p>Currently, the following objects are cached:</p>
<ul class="simple">
<li><dl class="first docutils">
<dt>datasets</dt>
<dd><ul class="first last">
<li>dataset arff. In order to reduce parsing time, the data is serialized to
disk in a binary format (using the <a class="reference external" href="https://docs.python.org/2/library/pickle.html">pickle library</a>.</li>
<li>dataset descriptions</li>
<li>more?</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>tasks</dt>
<dd><ul class="first last">
<li>task description</li>
<li>split arff. TODO are they cached?</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>runs</dt>
<dd><ul class="first last">
<li>run description</li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Run predictions are not cached yet. Flow ojects cannot yet be downloaded and are
therefore not cached.</p>
<div class="section" id="configuring-the-cache">
<h3>Configuring the cache<a class="headerlink" href="#configuring-the-cache" title="Permalink to this headline">¶</a></h3>
<p>Configuring the cache works as described in the subsection <a class="reference internal" href="#connecting-to-the-openml-server">Connecting to the OpenML server</a>:
It can be done either through the API:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">openml</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">set_cache_directory</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/.openml/cache&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>or the config file:</p>
<div class="code bash highlight-default"><div class="highlight"><pre><span></span><span class="n">cachedir</span> <span class="o">=</span> <span class="s1">&#39;~/.openml/cache&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="clearing-the-cache">
<h3>Clearing the cache<a class="headerlink" href="#clearing-the-cache" title="Permalink to this headline">¶</a></h3>
<p>Currently, there is no programmatic way to interact with the cache and we do not
plan to implement one. If you have any use case for this, please open an issue
on the <a class="reference external" href="https://github.com/openml/openml-python/issues">issue tracker</a>.</p>
<p># TODO check that the cache is in a consistent state!
In case the cache gets too large, you can manually delete unnecessary files.
Make sure that you always delete a complete entity, for example the whole
directory caching a dataset named after the datasets ID.</p>
</div>
</div>
<div class="section" id="working-with-flows-and-runs">
<h2>Working with Flows and Runs<a class="headerlink" href="#working-with-flows-and-runs" title="Permalink to this headline">¶</a></h2>
<p>Tasks and datasets allow us to download all information to run an experiment
locally. In order to upload and share results of such an experiment we need
the concepts of flows and runs.</p>
<p>Flows are descriptions of something runable which does the machine learning.
A flow contains all information to set up the necessary machine learning
library and its dependencies as well as all possible parameters.</p>
<p>A run is the outcome of running a flow on a task. It contains all parameter
settings for the flow, a setup string (most likely a command line call) and all
predictions of that run. When a run is uploaded to the server, the server
automatically calculates several metrics which can be used to compare the
performance of different flows to each other.</p>
<p>So far, the OpenML python connector works only with estimator objects following
the <a class="reference external" href="http://scikit-learn.org/dev/developers/contributing.html#apis-of-scikit-learn-objects">scikit-learn estimator API</a>.
Those can be directly run on a task, and a flow will automatically be created or
downloaded from the server if it already exists.</p>
<div class="section" id="running-a-model">
<h3>Running a model<a class="headerlink" href="#running-a-model" title="Permalink to this headline">¶</a></h3>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task</span> <span class="o">=</span> <span class="n">openml</span><span class="o">.</span><span class="n">tasks</span><span class="o">.</span><span class="n">get_task</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">run</span> <span class="o">=</span> <span class="n">openml</span><span class="o">.</span><span class="n">runs</span><span class="o">.</span><span class="n">run_model_on_task</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pprint</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="n">run</span><span class="p">),</span> <span class="n">depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>                             
</pre></div>
</div>
<p>So far the run is only available locally. By calling the publish function, the
run is send to the OpenML server:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">run</span><span class="o">.</span><span class="n">publish</span><span class="p">()</span>                                          
<span class="go"># What happens here? What should it return?</span>
</pre></div>
</div>
<p>We can now also inspect the flow object which was automatically created:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">flow</span> <span class="o">=</span> <span class="n">openml</span><span class="o">.</span><span class="n">flows</span><span class="o">.</span><span class="n">get_flow</span><span class="p">(</span><span class="n">run</span><span class="o">.</span><span class="n">flow_id</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pprint</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="n">flow</span><span class="p">),</span> <span class="n">depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">{&#39;binary_format&#39;: None,</span>
<span class="go"> &#39;binary_md5&#39;: None,</span>
<span class="go"> &#39;binary_url&#39;: None,</span>
<span class="go"> &#39;class_name&#39;: &#39;sklearn.ensemble.forest.RandomForestClassifier&#39;,</span>
<span class="go"> &#39;components&#39;: OrderedDict(),</span>
<span class="go"> &#39;custom_name&#39;: None,</span>
<span class="go"> &#39;dependencies&#39;: &#39;sklearn==0.18.2\nnumpy&gt;=1.6.1\nscipy&gt;=0.9&#39;,</span>
<span class="go"> &#39;description&#39;: &#39;Automatically created scikit-learn flow.&#39;,</span>
<span class="go"> &#39;external_version&#39;: &#39;openml==0.6.0dev,sklearn==0.18.2&#39;,</span>
<span class="go"> &#39;flow_id&#39;: 7245,</span>
<span class="go"> &#39;language&#39;: &#39;English&#39;,</span>
<span class="go"> &#39;model&#39;: RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,</span>
<span class="go">            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,</span>
<span class="go">            min_impurity_split=1e-07, min_samples_leaf=1,</span>
<span class="go">            min_samples_split=2, min_weight_fraction_leaf=0.0,</span>
<span class="go">            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,</span>
<span class="go">            verbose=0, warm_start=False),</span>
<span class="go"> &#39;name&#39;: &#39;sklearn.ensemble.forest.RandomForestClassifier&#39;,</span>
<span class="go"> &#39;parameters&#39;: OrderedDict([...]),</span>
<span class="go"> &#39;parameters_meta_info&#39;: OrderedDict([...]),</span>
<span class="go"> &#39;tags&#39;: [&#39;openml-python&#39;,</span>
<span class="go">          &#39;python&#39;,</span>
<span class="go">          &#39;scikit-learn&#39;,</span>
<span class="go">          &#39;sklearn&#39;,</span>
<span class="go">          &#39;sklearn_0.18.2&#39;],</span>
<span class="go"> &#39;upload_date&#39;: &#39;2017-10-06T14:54:38&#39;,</span>
<span class="go"> &#39;uploader&#39;: &#39;86&#39;,</span>
<span class="go"> &#39;version&#39;: &#39;28&#39;}</span>
</pre></div>
</div>
</div>
<div class="section" id="retrieving-results-from-openml">
<h3>Retrieving results from OpenML<a class="headerlink" href="#retrieving-results-from-openml" title="Permalink to this headline">¶</a></h3>
<p># TODO</p>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2014-2016, Matthias Feurer, Andreas Müller, Farzan Majdani, Joaquin Vanschoren and Pieter Gijsbers.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.4.<br/>
    </p>
  </div>
</footer>
  </body>
</html>